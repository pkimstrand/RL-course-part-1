{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopia av Session 2 SMARTER_RL_1-Value_Policy_Iteration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4fPp_O1t95q"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw7a3Ph5uAOK"
      },
      "source": [
        "# Preliminaries\n",
        "\n",
        "This notebook lets you import a gym environment and set up an agent that acts within the environment. Your tasks is to then implement some of the classical RL algorithms: Value iteration and Policy iteration. Play attention to how you are going to evaluate your agents.\n",
        "\n",
        "First, we make sure that all dependencies are met"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qktWZzEdtxNN"
      },
      "source": [
        "!pip install gym > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KLURrYHycqD"
      },
      "source": [
        "# Testing the Gym environments\n",
        "\n",
        "Our next step is to import the gym package, create an environment, and make sure that we can use it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlAGn42vyoyN",
        "outputId": "678b5e66-50e4-496a-dadd-29535ddebb10"
      },
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#create a cliff-walker\n",
        "env = gym.make('CliffWalking-v0')\n",
        "\n",
        "#set the start state\n",
        "state = env.reset()\n",
        "#and take some random actions\n",
        "for i in range(4):\n",
        "  #render the environment\n",
        "  env.render()\n",
        "  \n",
        "  #select a random action\n",
        "  action = env.action_space.sample()\n",
        "  #take a step and record next state, reward and termination\n",
        "  state, reward, done, _ = env.step(action)\n",
        "  print(\"Acted: {}\".format(action))\n",
        "  print(\"State: {}\".format(state))\n",
        "  print(\"Reward: {}\".format(reward))\n",
        "  if done:\n",
        "    #this environment only terminates once the goal is reached\n",
        "    print(\"Done.\")\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 1\n",
            "State: 36\n",
            "Reward: -100\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 3\n",
            "State: 36\n",
            "Reward: -1\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 3\n",
            "State: 36\n",
            "Reward: -1\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 1\n",
            "State: 36\n",
            "Reward: -100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3w4IfOE2Nck",
        "outputId": "5f778746-e34b-489f-e3bc-b8b71cb2d4bf"
      },
      "source": [
        "\n",
        "env.P[47]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 35, -1, False)],\n",
              " 1: [(1.0, 47, -1, True)],\n",
              " 2: [(1.0, 47, -1, True)],\n",
              " 3: [(1.0, 36, -100, False)]}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsWsM--l5D25"
      },
      "source": [
        "# Defining an agent\n",
        "\n",
        "The next step is to define a class for our agents. We will derive from this class to later implement a Value Iteration, Policy Iteration and Monte Carlo control agent. The base class will only provide simple functionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzIFdhOk5VoR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a502050c-f56e-4caa-cc41-772358b02d75"
      },
      "source": [
        "class Agent :\n",
        "  def __init__(self,env,discount_factor):\n",
        "    self.env = env\n",
        "    self.gamma = discount_factor\n",
        "  \n",
        "  def act(self, state):\n",
        "    return self.env.action_space.sample() #returns a random action\n",
        "\n",
        "  def evaluate(self):\n",
        "    # now let's test our random action agent\n",
        "    n_steps = 100 #number of steps per episode\n",
        "\n",
        "    s = env.reset()\n",
        "    episode_reward = 0\n",
        "    \n",
        "    for i in range(n_steps):\n",
        "      s, r, d, _ = env.step(self.act(s))\n",
        "      episode_reward += r\n",
        "      if d:\n",
        "        break\n",
        "    return episode_reward\n",
        "\n",
        "#test simple evaluation function\n",
        "random_agent = Agent(env,0.99)\n",
        "episode_reward=random_agent.evaluate()\n",
        "print(\"Episode return {}\".format(episode_reward))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode return -1783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY-nBquS4S4j"
      },
      "source": [
        "# Value Iteration Agent\n",
        "\n",
        "In this section you are to implement an agent that solves the environment, using Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYLjRZPg4cQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b13eb250-4096-4466-e492-29658abbfd22"
      },
      "source": [
        "class ValueAgent(Agent):\n",
        "  def __init__(self,env,discount_factor,theta):\n",
        "    super().__init__(env,discount_factor)\n",
        "    #theta is an approximation error threshold\n",
        "    self.theta = theta\n",
        "    self.V = np.random.rand(self.env.observation_space.n)\n",
        "    #set terminal state to 0\n",
        "    self.V[-1] = 0\n",
        "  \n",
        "  def act(self, state): \n",
        "    #here choose action that would bring us to state with highest value\n",
        "    values=[]\n",
        "    for i in range(self.env.nA):\n",
        "      v=0\n",
        "      for j in range(len(env.P[state][i])):\n",
        "        prob, next_state, reward, done = env.P[state][i][j]\n",
        "        v += reward + self.gamma*self.V[next_state]\n",
        "      values.append(v)\n",
        "    \n",
        "    action = np.argmax(values)\n",
        "    if (type(action)==np.array): print (action)\n",
        "    return action\n",
        "\n",
        "   \n",
        "\n",
        "  def iterate(self):\n",
        "    while(True):\n",
        "      delta = 0.0\n",
        "      for state in range(self.env.nS-1):\n",
        "        v = self.V[state]\n",
        "        action = self.act(state)\n",
        "        value = 0\n",
        "        for j in range(len(env.P[state][action])):\n",
        "          prob, next_state, reward, done = self.env.P[state][action][j]\n",
        "          value += prob * (reward + self.gamma*self.V[next_state])\n",
        "        self.V[state] = value\n",
        "        delta = max([delta, np.abs(v-self.V[state])])\n",
        "      print(delta)\n",
        "      if (delta < self.theta):\n",
        "        print(delta)\n",
        "        break\n",
        "\n",
        "\n",
        "agent = ValueAgent(env,0.99,0.001)\n",
        "print(agent.V[:12])\n",
        "print(agent.V[12:24])\n",
        "print(agent.V[24:36])\n",
        "print(agent.V[36:])\n",
        "#perform value iteration\n",
        "agent.iterate()\n",
        "#evaluate agent and plot relevant qualities\n",
        "episode_reward=agent.evaluate()\n",
        "print(\"Episode return {}\".format(episode_reward))\n",
        "np.set_printoptions(precision=3, linewidth=200)\n",
        "print(agent.V[:12])\n",
        "print(agent.V[12:24])\n",
        "print(agent.V[24:36])\n",
        "print(agent.V[36:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.46223671 0.25684455 0.19497257 0.3056002  0.05850981 0.26538718\n",
            " 0.09940028 0.17186533 0.34198701 0.27755251 0.59974448 0.93047607]\n",
            "[1.44170801e-01 8.27591536e-01 2.43928019e-01 5.37591319e-01\n",
            " 5.00579445e-01 2.51550393e-01 5.67495723e-04 7.04946268e-01\n",
            " 5.83921930e-01 8.85068984e-01 1.11293850e-01 8.20413242e-01]\n",
            "[0.73032607 0.81293906 0.88875561 0.08099286 0.12760931 0.09946621\n",
            " 0.1845644  0.58056678 0.7167344  0.37649339 0.27969584 0.88737672]\n",
            "[0.12975515 0.77636782 0.47056165 0.6169239  0.68605769 0.90463783\n",
            " 0.62139236 0.93365853 0.80285221 0.88560684 0.24507194 0.        ]\n",
            "2.713746259756124\n",
            "1.7904869481518353\n",
            "1.4822449503966837\n",
            "1.4674225008927166\n",
            "1.4527482758837893\n",
            "1.4382207931249518\n",
            "1.4238385851937023\n",
            "1.4096001993417655\n",
            "1.3955041973483473\n",
            "0.9220173068321298\n",
            "0.9127971337638083\n",
            "0.9036691624261692\n",
            "0.8946324708019073\n",
            "0.8856861460938887\n",
            "0.8083471863971727\n",
            "0.0\n",
            "0.0\n",
            "Episode return -13\n",
            "[-13.125 -12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97 ]\n",
            "[-12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97   -1.99 ]\n",
            "[-11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97   -1.99   -1.   ]\n",
            "[-12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -1.      0.   ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDDxB_GN__Tw"
      },
      "source": [
        "# Policy Iteration Agent\n",
        "Follow the same procedure for implementing a policy iteration agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gqQ38UqARau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9398193f-2081-45c4-9918-130ed3134226"
      },
      "source": [
        "class PolicyAgent(Agent):\n",
        "  def __init__(self,env,discount_factor,theta):\n",
        "    super().__init__(env,discount_factor)\n",
        "    #theta is an approximation error threshold\n",
        "    self.theta = theta\n",
        "    self.V = np.random.rand(self.env.observation_space.n)\n",
        "    #set terminal state to 0\n",
        "    #self.V[-11:-1] = -1000 \n",
        "    self.V[-1] = 0\n",
        "    self.policy = np.random.randint(4, size=self.env.observation_space.n)\n",
        "  \n",
        "  def act(self, state): \n",
        "    #here choose action that would bring us to state with highest value\n",
        "    values=[]\n",
        "    for i in range(self.env.nA):\n",
        "      v=0\n",
        "      for j in range(len(env.P[state][i])):\n",
        "        prob, next_state, reward, done = env.P[state][i][j]\n",
        "        v += reward + self.gamma*self.V[next_state]\n",
        "      values.append(v)\n",
        "    \n",
        "    action = np.argmax(values)\n",
        "    if (type(action)==np.array): print (action)\n",
        "    return action\n",
        "\n",
        "   \n",
        "\n",
        "  def evaluate_policy(self):\n",
        "    while(True):\n",
        "      delta = 0.0\n",
        "      for state in range(self.env.nS-1):\n",
        "        v = self.V[state]\n",
        "        action = self.policy[state]\n",
        "        value = 0\n",
        "        for j in range(len(env.P[state][action])):\n",
        "          prob, next_state, reward, done = self.env.P[state][action][j]\n",
        "          value += prob * (reward + self.gamma*self.V[next_state])\n",
        "        self.V[state] = value\n",
        "        delta = max([delta, np.abs(v-self.V[state])])\n",
        "      #print(delta)\n",
        "      if (delta < self.theta):\n",
        "        #print(delta)\n",
        "        break\n",
        "\n",
        "  def improve(self):\n",
        "    policy_stable = True\n",
        "    #for i in range(200):\n",
        "    while(True):\n",
        "      policy_stable = True\n",
        "      for state in range(self.env.nS-1):\n",
        "        old_action = self.policy[state]\n",
        "        self.policy[state] = self.act(state)\n",
        "        if not (self.policy[state] == old_action):\n",
        "          policy_stable = False\n",
        "      if policy_stable:\n",
        "        break\n",
        "      else:\n",
        "        self.evaluate_policy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "agent = PolicyAgent(env,0.99,0.001)\n",
        "print(agent.V[:12])\n",
        "print(agent.V[12:24])\n",
        "print(agent.V[24:36])\n",
        "print(agent.V[36:])\n",
        "print(agent.policy[:12])\n",
        "print(agent.policy[12:24])\n",
        "print(agent.policy[24:36])\n",
        "print(agent.policy[36:])\n",
        "#perform value iteration\n",
        "agent.improve()\n",
        "#evaluate agent and plot relevant qualities\n",
        "episode_reward=agent.evaluate()\n",
        "print(\"Episode return {}\".format(episode_reward))\n",
        "np.set_printoptions(precision=3, linewidth=200)\n",
        "print(agent.V[:12])\n",
        "print(agent.V[12:24])\n",
        "print(agent.V[24:36])\n",
        "print(agent.V[36:])\n",
        "\n",
        "print(agent.policy[:12])\n",
        "print(agent.policy[12:24])\n",
        "print(agent.policy[24:36])\n",
        "print(agent.policy[36:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.62  0.038 0.394 0.641 0.63  0.001 0.778 0.652 0.477 0.39  0.317 0.127]\n",
            "[0.797 0.394 0.321 0.839 0.143 0.684 0.495 0.748 0.139 0.081 0.348 0.629]\n",
            "[0.503 0.344 0.746 0.549 0.756 0.772 0.263 0.929 0.954 0.536 0.897 0.045]\n",
            "[0.442 0.779 0.972 0.627 0.917 0.191 0.324 0.112 0.465 0.386 0.467 0.   ]\n",
            "[0 3 2 2 0 2 0 3 2 0 1 2]\n",
            "[2 1 3 1 1 0 2 1 3 1 1 1]\n",
            "[2 2 1 2 3 3 2 1 0 0 0 1]\n",
            "[0 2 2 1 1 2 0 3 0 1 2 1]\n",
            "Episode return -13\n",
            "[-13.125 -12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97 ]\n",
            "[-12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97   -1.99 ]\n",
            "[-11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97   -1.99   -1.   ]\n",
            "[-12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -1.      0.   ]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 2]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 2]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 2]\n",
            "[0 0 0 0 0 0 0 0 0 0 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieprV4kW3mZC"
      },
      "source": [
        "Modifying the transisiotn probabilities to create a non-deterministic environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u69f8gyXAGN_"
      },
      "source": [
        "#Monte Carlo control agent\n",
        "Follow the same procedure for implementing a Monte Carlo control agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KwYvXP4ASSJ"
      },
      "source": [
        "#code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}