{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 2 SMARTER_RL_1-Value_Policy_Iteration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4fPp_O1t95q"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw7a3Ph5uAOK"
      },
      "source": [
        "# Preliminaries\n",
        "\n",
        "This notebook lets you import a gym environment and set up an agent that acts within the environment. Your tasks is to then implement some of the classical RL algorithms: Value iteration and Policy iteration. Play attention to how you are going to evaluate your agents.\n",
        "\n",
        "First, we make sure that all dependencies are met"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qktWZzEdtxNN"
      },
      "source": [
        "!pip install gym > /dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KLURrYHycqD"
      },
      "source": [
        "# Testing the Gym environments\n",
        "\n",
        "Our next step is to import the gym package, create an environment, and make sure that we can use it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlAGn42vyoyN",
        "outputId": "9871bdcf-1ec0-4b28-c680-f0652108ef8a"
      },
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#create a cliff-walker\n",
        "env = gym.make('CliffWalking-v0')\n",
        "\n",
        "#set the start state\n",
        "state = env.reset()\n",
        "#and take some random actions\n",
        "for i in range(4):\n",
        "  #render the environment\n",
        "  env.render()\n",
        "  \n",
        "  #select a random action\n",
        "  action = env.action_space.sample()\n",
        "  #take a step and record next state, reward and termination\n",
        "  state, reward, done, _ = env.step(action)\n",
        "  print(\"Acted: {}\".format(action))\n",
        "  print(\"State: {}\".format(state))\n",
        "  print(\"Reward: {}\".format(reward))\n",
        "  if done:\n",
        "    #this environment only terminates once the goal is reached\n",
        "    print(\"Done.\")\n",
        "    break"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 1\n",
            "State: 36\n",
            "Reward: -100\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 2\n",
            "State: 36\n",
            "Reward: -1\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 0\n",
            "State: 24\n",
            "Reward: -1\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 0\n",
            "State: 12\n",
            "Reward: -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsWsM--l5D25"
      },
      "source": [
        "# Defining an agent\n",
        "\n",
        "The next step is to define a class for our agents. We will derive from this class to later implement a Value Iteration, Policy Iteration and Monte Carlo control agent. The base class will only provide simple functionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzIFdhOk5VoR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "537f916b-63b0-49f4-9be7-fe431a05c62d"
      },
      "source": [
        "class Agent :\n",
        "  def __init__(self,env,discount_factor):\n",
        "    self.env = env\n",
        "    self.gamma = discount_factor\n",
        "  \n",
        "  def act(self, state):\n",
        "    return self.env.action_space.sample() #returns a random action\n",
        "\n",
        "  def evaluate(self):\n",
        "    # now let's test our random action agent\n",
        "    n_steps = 100 #number of steps per episode\n",
        "\n",
        "    s = env.reset()\n",
        "    episode_reward = 0\n",
        "    \n",
        "    for i in range(n_steps):\n",
        "      s, r, d, _ = env.step(self.act(s))\n",
        "      episode_reward += r\n",
        "      if d:\n",
        "        break\n",
        "    return episode_reward\n",
        "\n",
        "#test simple evaluation function\n",
        "random_agent = Agent(env,0.99)\n",
        "episode_reward=random_agent.evaluate()\n",
        "print(\"Episode return {}\".format(episode_reward))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode return -1387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY-nBquS4S4j"
      },
      "source": [
        "# Value Iteration Agent\n",
        "\n",
        "In this section you are to implement an agent that solves the environment, using Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYLjRZPg4cQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d11f04-1fd6-45e2-d22c-026185e69182"
      },
      "source": [
        "class ValueAgent(Agent):\n",
        "  def __init__(self,env,discount_factor,theta):\n",
        "    super().__init__(env,discount_factor)\n",
        "    #theta is an approximation error threshold\n",
        "    self.theta = theta\n",
        "    self.V = np.random.rand(self.env.shape[0], self.env.shape[1])\n",
        "    #set terminal state to 0\n",
        "    self.V[-1,-1] = 0\n",
        "  \n",
        "  def act(self, state): \n",
        "    #here choose action that would bring us to state with highest value\n",
        "    values=[]\n",
        "    for i in range(self.env.nA):\n",
        "      v=0\n",
        "      for j in range(len(env.P[state][i])):\n",
        "        prob, next_state, reward, done = env.P[state][i][j]\n",
        "        next_position = np.unravel_index(next_state, self.env.shape)\n",
        "        v += reward + self.gamma*self.V[next_position]\n",
        "      values.append(v)\n",
        "    \n",
        "    action = np.argmax(values)\n",
        "    if (type(action)==np.array): print (action)\n",
        "    return action\n",
        "\n",
        "   \n",
        "\n",
        "  def iterate(self):\n",
        "    while(True):\n",
        "      delta = 0.0\n",
        "      for state in range(self.env.nS-1):\n",
        "        position = np.unravel_index(state, self.env.shape)\n",
        "        v = self.V[position]\n",
        "        action = self.act(state)\n",
        "        value = 0\n",
        "        for j in range(len(env.P[state][action])):\n",
        "          prob, next_state, reward, done = self.env.P[state][action][j]\n",
        "          next_position = np.unravel_index(next_state, self.env.shape)\n",
        "          value += prob * (reward + self.gamma*self.V[next_position])\n",
        "        self.V[position] = value\n",
        "        delta = max([delta, np.abs(v-self.V[position])])\n",
        "      print(delta)\n",
        "      if (delta < self.theta):\n",
        "        print(delta)\n",
        "        break\n",
        "\n",
        "np.set_printoptions(precision=3, linewidth=200)\n",
        "\n",
        "agent = ValueAgent(env,0.99,0.001)\n",
        "print(agent.V)\n",
        "#perform value iteration\n",
        "agent.iterate()\n",
        "#evaluate agent and plot relevant qualities\n",
        "episode_reward=agent.evaluate()\n",
        "print(\"Episode return {}\".format(episode_reward))\n",
        "\n",
        "print(agent.V)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.733 0.018 0.813 0.48  0.47  0.784 0.332 0.432 0.061 0.743 0.494 0.451]\n",
            " [0.844 0.065 0.237 0.095 0.276 0.247 0.857 0.574 0.779 0.696 0.957 0.972]\n",
            " [0.121 0.337 0.367 0.15  0.674 0.457 0.063 0.027 0.114 0.297 0.422 0.91 ]\n",
            " [0.268 0.428 0.84  0.969 0.665 0.588 0.957 0.033 0.293 0.841 0.939 0.   ]]\n",
            "2.9200050072988297\n",
            "1.3800795863819826\n",
            "1.3662787905181628\n",
            "1.3526160026129816\n",
            "1.3390898425868518\n",
            "1.226340800469469\n",
            "1.0266721934966148\n",
            "1.0164054715616482\n",
            "0.931713174529043\n",
            "0.9223960427837534\n",
            "0.9131720823559153\n",
            "0.9040403615323562\n",
            "0.8949999579170314\n",
            "0.8860499583378623\n",
            "0.844364598550488\n",
            "0.0\n",
            "0.0\n",
            "Episode return -13\n",
            "[[-13.125 -12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97 ]\n",
            " [-12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97   -1.99 ]\n",
            " [-11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97   -1.99   -1.   ]\n",
            " [-12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -1.      0.   ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDDxB_GN__Tw"
      },
      "source": [
        "# Policy Iteration Agent\n",
        "Follow the same procedure for implementing a policy iteration agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gqQ38UqARau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ade699d-b934-4cc6-f86e-94d9446b1560"
      },
      "source": [
        "class PolicyAgent(Agent):\n",
        "  def __init__(self,env,discount_factor,theta):\n",
        "    super().__init__(env,discount_factor)\n",
        "    #theta is an approximation error threshold\n",
        "    self.theta = theta\n",
        "    self.V = np.random.rand(self.env.shape[0], self.env.shape[1])\n",
        "    #set terminal state to 0\n",
        "    self.V[-1,-1] = 0\n",
        "    self.policy = np.random.randint(4, size=self.env.shape)\n",
        "  \n",
        "  def act(self, state): \n",
        "    #here choose action that would bring us to state with highest value\n",
        "    values=[]\n",
        "    for i in range(self.env.nA):\n",
        "      v=0\n",
        "      for j in range(len(env.P[state][i])):\n",
        "        prob, next_state, reward, done = env.P[state][i][j]\n",
        "        next_position = np.unravel_index(next_state, self.env.shape)\n",
        "        v += reward + self.gamma*self.V[next_position]\n",
        "      values.append(v)\n",
        "    \n",
        "    action = np.argmax(values)\n",
        "    if (type(action)==np.array): print (action)\n",
        "    return action\n",
        "\n",
        "   \n",
        "\n",
        "  def evaluate_policy(self):\n",
        "    while(True):\n",
        "      delta = 0.0\n",
        "      for state in range(self.env.nS-1):\n",
        "        position = np.unravel_index(state, self.env.shape)\n",
        "        v = self.V[position]\n",
        "        action = self.policy[position]\n",
        "        value = 0\n",
        "        for j in range(len(env.P[state][action])):\n",
        "          prob, next_state, reward, done = self.env.P[state][action][j]\n",
        "          next_position = np.unravel_index(next_state, self.env.shape)\n",
        "          value += prob * (reward + self.gamma*self.V[next_position])\n",
        "        self.V[position] = value\n",
        "        delta = max([delta, np.abs(v-self.V[position])])\n",
        "      #print(delta)\n",
        "      if (delta < self.theta):\n",
        "        #print(delta)\n",
        "        break\n",
        "\n",
        "  def improve(self):\n",
        "    policy_stable = True\n",
        "    #for i in range(200):\n",
        "    while(True):\n",
        "      policy_stable = True\n",
        "      for state in range(self.env.nS-1):\n",
        "        position = np.unravel_index(state, self.env.shape)\n",
        "        old_action = self.policy[position]\n",
        "        self.policy[position] = self.act(state)\n",
        "        if not (self.policy[position] == old_action):\n",
        "          policy_stable = False\n",
        "      if policy_stable:\n",
        "        break\n",
        "      else:\n",
        "        self.evaluate_policy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "agent = PolicyAgent(env,0.99,0.001)\n",
        "print(agent.V)\n",
        "\n",
        "print(agent.policy)\n",
        "#perform value iteration\n",
        "agent.improve()\n",
        "#evaluate agent and plot relevant qualities\n",
        "episode_reward=agent.evaluate()\n",
        "print(\"Episode return {}\".format(episode_reward))\n",
        "np.set_printoptions(precision=3, linewidth=200)\n",
        "print(agent.V)\n",
        "\n",
        "print(agent.policy)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.815 0.366 0.065 0.349 0.204 0.262 0.921 0.389 0.414 0.5   0.034 0.062]\n",
            " [0.497 0.24  0.617 0.461 0.992 0.602 0.378 0.842 0.234 0.693 0.563 0.414]\n",
            " [0.726 0.342 0.655 0.282 0.184 0.533 0.38  0.094 0.556 0.787 0.122 0.224]\n",
            " [0.4   0.221 0.105 0.263 0.728 0.009 0.538 0.016 0.796 0.87  0.871 0.   ]]\n",
            "[[1 0 0 2 0 3 1 1 2 3 0 3]\n",
            " [3 3 1 2 0 0 2 1 1 0 3 2]\n",
            " [1 3 2 2 0 3 2 0 3 3 3 2]\n",
            " [0 1 1 2 3 0 0 1 1 0 1 2]]\n",
            "Episode return -13\n",
            "[[-13.125 -12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97 ]\n",
            " [-12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97   -1.99 ]\n",
            " [-11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97   -1.99   -1.   ]\n",
            " [-12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -1.      0.   ]]\n",
            "[[1 1 1 1 1 1 1 1 1 1 1 2]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 2]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 2]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6TkdlfzPpcI"
      },
      "source": [
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u69f8gyXAGN_"
      },
      "source": [
        "#Monte Carlo control agent\n",
        "Follow the same procedure for implementing a Monte Carlo control agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KwYvXP4ASSJ"
      },
      "source": [
        "#code here"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}