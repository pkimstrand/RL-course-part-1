{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 2 SMARTER_RL_1-Value_Policy_Iteration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4fPp_O1t95q"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw7a3Ph5uAOK"
      },
      "source": [
        "# Preliminaries\n",
        "\n",
        "This notebook lets you import a gym environment and set up an agent that acts within the environment. Your tasks is to then implement some of the classical RL algorithms: Value iteration and Policy iteration. Play attention to how you are going to evaluate your agents.\n",
        "\n",
        "First, we make sure that all dependencies are met"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qktWZzEdtxNN"
      },
      "source": [
        "!pip install gym > /dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KLURrYHycqD"
      },
      "source": [
        "# Testing the Gym environments\n",
        "\n",
        "Our next step is to import the gym package, create an environment, and make sure that we can use it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlAGn42vyoyN",
        "outputId": "b7fa0de4-f610-4720-f06a-e07b96373820"
      },
      "source": [
        "%matplotlib notebook\n",
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#create a cliff-walker\n",
        "env = gym.make('CliffWalking-v0')\n",
        "\n",
        "#set the start state\n",
        "state = env.reset()\n",
        "#and take some random actions\n",
        "for i in range(4):\n",
        "  #render the environment\n",
        "  env.render()\n",
        "  \n",
        "  #select a random action\n",
        "  action = env.action_space.sample()\n",
        "  #take a step and record next state, reward and termination\n",
        "  state, reward, done, _ = env.step(action)\n",
        "  print(\"Acted: {}\".format(action))\n",
        "  print(\"State: {}\".format(state))\n",
        "  print(\"Reward: {}\".format(reward))\n",
        "  if done:\n",
        "    #this environment only terminates once the goal is reached\n",
        "    print(\"Done.\")\n",
        "    break"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 0\n",
            "State: 24\n",
            "Reward: -1\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 3\n",
            "State: 24\n",
            "Reward: -1\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 1\n",
            "State: 25\n",
            "Reward: -1\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  x  o  o  o  o  o  o  o  o  o  o\n",
            "o  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 1\n",
            "State: 26\n",
            "Reward: -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsWsM--l5D25"
      },
      "source": [
        "# Defining an agent\n",
        "\n",
        "The next step is to define a class for our agents. We will derive from this class to later implement a Value Iteration, Policy Iteration and Monte Carlo control agent. The base class will only provide simple functionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzIFdhOk5VoR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a7edd66-e0bc-4c6c-93bc-394ccfac8699"
      },
      "source": [
        "class Agent :\n",
        "  def __init__(self,env,discount_factor):\n",
        "    self.env = env\n",
        "    self.gamma = discount_factor\n",
        "  \n",
        "  def act(self, state):\n",
        "    return self.env.action_space.sample() #returns a random action\n",
        "\n",
        "  def evaluate(self):\n",
        "    # now let's test our random action agent\n",
        "    n_steps = 100 #number of steps per episode\n",
        "\n",
        "    s = env.reset()\n",
        "    episode_reward = 0\n",
        "    \n",
        "    for i in range(n_steps):\n",
        "      s, r, d, _ = env.step(self.act(s))\n",
        "      episode_reward += r\n",
        "      if d:\n",
        "        break\n",
        "    return episode_reward\n",
        "\n",
        "#test simple evaluation function\n",
        "random_agent = Agent(env,0.99)\n",
        "episode_reward=random_agent.evaluate()\n",
        "print(\"Episode return {}\".format(episode_reward))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode return -397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY-nBquS4S4j"
      },
      "source": [
        "# Value Iteration Agent\n",
        "\n",
        "In this section you are to implement an agent that solves the environment, using Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYLjRZPg4cQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec22f46-f226-4f24-aaa3-a3f49befd14c"
      },
      "source": [
        "class ValueAgent(Agent):\n",
        "  def __init__(self,env,discount_factor,theta):\n",
        "    super().__init__(env,discount_factor)\n",
        "    #theta is an approximation error threshold\n",
        "    self.theta = theta\n",
        "    self.V = np.random.rand(self.env.shape[0], self.env.shape[1])\n",
        "    #set terminal state to 0\n",
        "    self.V[-1,-1] = 0\n",
        "  \n",
        "  def act(self, state): \n",
        "    #here choose action that would bring us to state with highest value\n",
        "    values=[]\n",
        "    for i in range(self.env.nA):\n",
        "      v=0\n",
        "      for j in range(len(self.env.P[state][i])):\n",
        "        prob, next_state, reward, done = self.env.P[state][i][j]\n",
        "        next_position = np.unravel_index(next_state, self.env.shape)\n",
        "        v += prob*(reward + self.gamma*self.V[next_position])\n",
        "      values.append(v)\n",
        "    \n",
        "    action = np.argmax(values)\n",
        "    if (type(action)==np.array): print (action)\n",
        "    return action\n",
        "\n",
        "   \n",
        "\n",
        "  def iterate(self):\n",
        "    while(True):\n",
        "      delta = 0.0\n",
        "      for state in range(self.env.nS-1):\n",
        "        position = np.unravel_index(state, self.env.shape)\n",
        "        v = self.V[position]\n",
        "        action = self.act(state)\n",
        "        value = 0\n",
        "        for j in range(len(self.env.P[state][action])):\n",
        "          prob, next_state, reward, done = self.env.P[state][action][j]\n",
        "          next_position = np.unravel_index(next_state, self.env.shape)\n",
        "          value += prob * (reward + self.gamma*self.V[next_position])\n",
        "        self.V[position] = value\n",
        "        delta = max([delta, np.abs(v-self.V[position])])\n",
        "      print(delta)\n",
        "      if (delta < self.theta):\n",
        "        print(delta)\n",
        "        break\n",
        "\n",
        "np.set_printoptions(precision=3, linewidth=200)\n",
        "\n",
        "agent = ValueAgent(env,0.99,0.001)\n",
        "print(agent.V)\n",
        "#perform value iteration\n",
        "agent.iterate()\n",
        "#evaluate agent and plot relevant qualities\n",
        "episode_reward=agent.evaluate()\n",
        "print(\"Episode return {}\".format(episode_reward))\n",
        "\n",
        "print(agent.V)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.012 0.426 0.891 0.428 0.27  0.132 0.523 0.869 0.422 0.013 0.586 0.623]\n",
            " [0.856 0.508 0.23  0.977 0.509 0.156 0.936 0.42  0.6   0.511 0.387 0.658]\n",
            " [0.515 0.822 0.62  0.746 0.189 0.141 0.102 0.682 0.759 0.351 0.834 0.27 ]\n",
            " [0.346 0.694 0.829 0.103 0.862 0.872 0.75  0.11  0.976 0.215 0.477 0.   ]]\n",
            "2.762843760363978\n",
            "1.5508861367848348\n",
            "1.5353772754169865\n",
            "1.5200235026628164\n",
            "1.5048232676361883\n",
            "1.489775034959826\n",
            "1.474877284610228\n",
            "1.4601285117641254\n",
            "1.4455272266464858\n",
            "1.4310719543800214\n",
            "1.0691553781654815\n",
            "0.9040815928148831\n",
            "0.8950407768867343\n",
            "0.8860903691178681\n",
            "0.8483652657710703\n",
            "0.0\n",
            "0.0\n",
            "Episode return -13\n",
            "[[-13.125 -12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97 ]\n",
            " [-12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97   -1.99 ]\n",
            " [-11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97   -1.99   -1.   ]\n",
            " [-12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -1.      0.   ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDDxB_GN__Tw"
      },
      "source": [
        "# Policy Iteration Agent\n",
        "Follow the same procedure for implementing a policy iteration agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gqQ38UqARau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef812a78-ec3f-4000-c0ae-f811650557da"
      },
      "source": [
        "class PolicyAgent(Agent):\n",
        "  def __init__(self,env,discount_factor,theta):\n",
        "    super().__init__(env,discount_factor)\n",
        "    #theta is an approximation error threshold\n",
        "    self.theta = theta\n",
        "    self.V = np.random.rand(self.env.shape[0], self.env.shape[1])\n",
        "    #set terminal state to 0\n",
        "    self.V[-1,-1] = 0\n",
        "    self.policy = np.random.randint(4, size=self.env.shape)\n",
        "  \n",
        "  def act(self, state): \n",
        "    #here choose action that would bring us to state with highest value\n",
        "    values=[]\n",
        "    for i in range(self.env.nA):\n",
        "      v=0\n",
        "      for j in range(len(self.env.P[state][i])):\n",
        "        prob, next_state, reward, done = self.env.P[state][i][j]\n",
        "        next_position = np.unravel_index(next_state, self.env.shape)\n",
        "        v += prob * (reward + self.gamma*self.V[next_position])\n",
        "      values.append(v)\n",
        "    \n",
        "    action = np.argmax(values)\n",
        "    if (type(action)==np.array): print (action)\n",
        "    return action\n",
        "\n",
        "   \n",
        "\n",
        "  def evaluate_policy(self):\n",
        "    while(True):\n",
        "      delta = 0.0\n",
        "      for state in range(self.env.nS-1):\n",
        "        position = np.unravel_index(state, self.env.shape)\n",
        "        v = self.V[position]\n",
        "        action = self.policy[position]\n",
        "        value = 0\n",
        "        for j in range(len(self.env.P[state][action])):\n",
        "          prob, next_state, reward, done = self.env.P[state][action][j]\n",
        "          next_position = np.unravel_index(next_state, self.env.shape)\n",
        "          value += prob * (reward + self.gamma*self.V[next_position])\n",
        "        self.V[position] = value\n",
        "        delta = max([delta, np.abs(v-self.V[position])])\n",
        "      #print(delta)\n",
        "      if (delta < self.theta):\n",
        "        #print(delta)\n",
        "        break\n",
        "\n",
        "  def improve(self):\n",
        "    policy_stable = True\n",
        "    #for i in range(200):\n",
        "    while(True):\n",
        "      policy_stable = True\n",
        "      for state in range(self.env.nS-1):\n",
        "        position = np.unravel_index(state, self.env.shape)\n",
        "        old_action = self.policy[position]\n",
        "        self.policy[position] = self.act(state)\n",
        "        if not (self.policy[position] == old_action):\n",
        "          policy_stable = False\n",
        "      if policy_stable:\n",
        "        break\n",
        "      else:\n",
        "        self.evaluate_policy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "agent = PolicyAgent(env,0.99,0.001)\n",
        "print(agent.V)\n",
        "\n",
        "print(agent.policy)\n",
        "#perform value iteration\n",
        "agent.improve()\n",
        "#evaluate agent and plot relevant qualities\n",
        "episode_reward=agent.evaluate()\n",
        "print(\"Episode return {}\".format(episode_reward))\n",
        "np.set_printoptions(precision=3, linewidth=200)\n",
        "print(agent.V)\n",
        "\n",
        "print(agent.policy)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.641 0.342 0.921 0.84  0.38  0.566 0.278 0.218 0.428 0.755 0.357 0.673]\n",
            " [0.378 0.439 0.53  0.379 0.904 0.24  0.987 0.418 0.026 0.232 0.879 0.325]\n",
            " [0.093 0.607 0.09  0.883 0.95  0.532 0.496 0.729 0.315 0.114 0.593 0.194]\n",
            " [0.689 0.934 0.675 0.865 0.717 0.578 0.886 0.177 0.922 0.817 0.068 0.   ]]\n",
            "[[0 1 1 1 1 0 1 1 0 1 3 0]\n",
            " [2 3 3 0 0 2 0 1 1 2 0 3]\n",
            " [0 3 2 1 1 0 2 1 1 3 2 2]\n",
            " [1 2 1 1 3 1 2 3 3 3 1 1]]\n",
            "Episode return -13\n",
            "[[-13.125 -12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97 ]\n",
            " [-12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97   -1.99 ]\n",
            " [-11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97   -1.99   -1.   ]\n",
            " [-12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -1.      0.   ]]\n",
            "[[1 1 1 1 1 1 1 1 1 1 1 2]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 2]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 2]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6TkdlfzPpcI"
      },
      "source": [
        "from gym.envs.toy_text.cliffwalking import CliffWalkingEnv\n",
        "import copy\n",
        "\n",
        "class NonDCliffWalkingEnv(CliffWalkingEnv):\n",
        "  def __init__(self):\n",
        "    super(NonDCliffWalkingEnv, self).__init__()\n",
        "  \n",
        "  def _calculate_transition_prob(self, current, delta):\n",
        "    \"\"\"\n",
        "    Determine the outcome for an action. Transition Prob is always 1.0.\n",
        "    :param current: Current position on the grid as (row, col)\n",
        "    :param delta: Change in position for transition\n",
        "    :return: (1.0, new_state, reward, done)\n",
        "    \"\"\"\n",
        "    transitions = []\n",
        "    terminal_state = (self.shape[0] - 1, self.shape[1] - 1)\n",
        "    deltas = [delta, [2*delta[0], 2*delta[1]]]\n",
        "    zero_index = delta.index(0)\n",
        "    delta_off = copy.copy(delta)\n",
        "    delta_off[zero_index] = 1\n",
        "    deltas.append(copy.copy(delta_off))\n",
        "    delta_off[zero_index] = -1\n",
        "    deltas.append(delta_off)\n",
        "    probabilities = [0.85,0.05,0.05,0.05]\n",
        "\n",
        "    \n",
        "    for d,p in zip(deltas, probabilities):\n",
        "      new_position = np.array(current) + np.array(d)\n",
        "      new_position = self._limit_coordinates(new_position).astype(int)\n",
        "      new_state = self.start_state_index if self._cliff[tuple(new_position)] else np.ravel_multi_index(tuple(new_position), self.shape)\n",
        "      reward = -100 if self._cliff[tuple(new_position)] else -1\n",
        "      is_done = tuple(new_position) == terminal_state\n",
        "      #print(tuple(new_position), self._cliff[tuple(new_position)])\n",
        "      transitions.append((p, new_state, reward, is_done))\n",
        "    \n",
        "    return transitions\n",
        "  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7b4tF5uHpT6"
      },
      "source": [
        "penv = NonDCliffWalkingEnv()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKYehYTOIshW",
        "outputId": "ac17b94a-7004-49d2-c749-07a5c2836662"
      },
      "source": [
        "agent = ValueAgent(penv,0.99,0.001)\n",
        "print(agent.V)\n",
        "#perform value iteration\n",
        "agent.iterate()\n",
        "#evaluate agent and plot relevant qualities\n",
        "episode_reward=agent.evaluate()\n",
        "print(\"Episode return {}\".format(episode_reward))\n",
        "\n",
        "print(agent.V)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.365 0.873 0.254 0.386 0.905 0.495 0.214 0.012 0.917 0.313 0.624 0.962]\n",
            " [0.64  0.148 0.632 0.914 0.171 0.167 0.356 0.038 0.844 0.703 0.076 0.105]\n",
            " [0.963 0.891 0.544 0.507 0.809 0.875 0.84  0.532 0.345 0.011 0.23  0.797]\n",
            " [0.427 0.947 0.297 0.3   0.083 0.327 0.706 0.53  0.624 0.683 0.327 0.   ]]\n",
            "3.4466326055733845\n",
            "1.6689345116641454\n",
            "1.5848090408697173\n",
            "1.3895449618879923\n",
            "1.3238920575528157\n",
            "1.2652613471889147\n",
            "1.2127562406416956\n",
            "1.1668226757445392\n",
            "1.1231294286688218\n",
            "0.9673417294492275\n",
            "0.955859307854741\n",
            "0.9446285087963702\n",
            "0.9336304871008121\n",
            "0.9228475972050365\n",
            "0.9122625171248924\n",
            "0.9018570717321328\n",
            "0.8861190025830403\n",
            "0.8760510896740605\n",
            "0.8584305423870155\n",
            "0.7222133959945438\n",
            "0.22571812050809115\n",
            "0.05591775103848562\n",
            "0.04002370896781571\n",
            "0.03922236717693828\n",
            "0.03882195580299985\n",
            "0.03842435816424761\n",
            "0.03802960637174202\n",
            "0.03763823595917515\n",
            "0.037248527336988246\n",
            "0.03681507782210147\n",
            "0.036032019780645186\n",
            "0.033349938634394505\n",
            "0.025232703464631356\n",
            "0.011714489190701727\n",
            "0.002874895295381208\n",
            "0.0016860017510360592\n",
            "0.001592681596473966\n",
            "0.001573921182185245\n",
            "0.0015577435630298453\n",
            "0.001541689770906629\n",
            "0.0015250483755231414\n",
            "0.0015034692826425555\n",
            "0.0014597816614774217\n",
            "0.001346603246130229\n",
            "0.001093436421847116\n",
            "0.0006957627032839753\n",
            "0.0006957627032839753\n",
            "Episode return -15\n",
            "[[-17.382 -16.593 -15.798 -14.999 -14.196 -13.389 -12.578 -11.765 -10.949 -10.132  -9.31   -8.445]\n",
            " [-17.326 -16.486 -15.635 -14.773 -13.897 -13.009 -12.108 -11.192 -10.262  -9.31   -8.445  -7.504]\n",
            " [-18.114 -17.326 -16.486 -15.635 -14.773 -13.897 -13.01  -12.108 -11.192 -10.262  -7.504  -6.883]\n",
            " [-18.855 -18.109 -17.279 -16.436 -15.581 -14.714 -13.834 -12.941 -12.034 -11.022  -1.341   0.   ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea0XuckqJI65",
        "outputId": "5bce8798-875f-4405-fb83-2a2b6215acb2"
      },
      "source": [
        "pagent = PolicyAgent(penv,0.99,0.001)\n",
        "print(agent.V)\n",
        "\n",
        "print(pagent.policy)\n",
        "#perform value iteration\n",
        "pagent.improve()\n",
        "#evaluate agent and plot relevant qualities\n",
        "episode_reward=pagent.evaluate()\n",
        "print(\"Episode return {}\".format(episode_reward))\n",
        "\n",
        "print(pagent.V)\n",
        "\n",
        "print(pagent.policy)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-17.382 -16.593 -15.798 -14.999 -14.196 -13.389 -12.578 -11.765 -10.949 -10.132  -9.31   -8.445]\n",
            " [-17.326 -16.486 -15.635 -14.773 -13.897 -13.009 -12.108 -11.192 -10.262  -9.31   -8.445  -7.504]\n",
            " [-18.114 -17.326 -16.486 -15.635 -14.773 -13.897 -13.01  -12.108 -11.192 -10.262  -7.504  -6.883]\n",
            " [-18.855 -18.109 -17.279 -16.436 -15.581 -14.714 -13.834 -12.941 -12.034 -11.022  -1.341   0.   ]]\n",
            "[[2 1 1 0 0 2 3 3 2 2 2 3]\n",
            " [2 2 2 2 3 0 2 1 1 3 0 3]\n",
            " [2 0 1 2 2 0 3 2 1 3 0 2]\n",
            " [2 1 1 1 0 0 3 2 1 2 1 2]]\n",
            "Episode return -15\n",
            "[[-17.384 -16.594 -15.8   -15.001 -14.198 -13.391 -12.58  -11.766 -10.95  -10.133  -9.311  -8.446]\n",
            " [-17.328 -16.488 -15.637 -14.774 -13.899 -13.011 -12.109 -11.194 -10.263  -9.311  -8.446  -7.505]\n",
            " [-18.116 -17.328 -16.488 -15.637 -14.774 -13.899 -13.011 -12.109 -11.194 -10.263  -7.505  -6.883]\n",
            " [-18.857 -18.111 -17.281 -16.437 -15.582 -14.715 -13.836 -12.942 -12.035 -11.023  -1.341   0.   ]]\n",
            "[[1 1 1 1 1 1 1 1 1 1 2 2]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 2]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 2]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u69f8gyXAGN_"
      },
      "source": [
        "#Monte Carlo control agent\n",
        "Follow the same procedure for implementing a Monte Carlo control agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KwYvXP4ASSJ"
      },
      "source": [
        "class MonteCarloAgent(Agent):\n",
        "  def __init__(self,env,discount_factor,epsilon):\n",
        "    super().__init__(env,discount_factor)\n",
        "    #theta is an approximation error threshold\n",
        "    self.epsilon = epsilon\n",
        "    self.Q = np.random.rand(self.env.shape[0], self.env.shape[1], self.env.nA)\n",
        "    #set terminal state to 0\n",
        "    self.Q[-1,-1,:] = 0\n",
        "    self.policy = np.random.rand(self.env.shape[0], self.env.shape[1], self.env.nA)\n",
        "    # Normalize to 1.0\n",
        "    for i in range(self.env.shape[0]):\n",
        "      for j in range(self.env.shape[1]):\n",
        "        self.policy[i,j,:] /= np.sum(self.policy[i,j,:].flatten())\n",
        "    self.returns = np.empty((self.env.shape[0], self.env.shape[1], self.env.nA), dtype=list)\n",
        "    for i in range(self.env.shape[0]):\n",
        "      for j in range(self.env.shape[1]):\n",
        "        for k in range(self.env.nA):\n",
        "          self.returns[i,j,k] = []\n",
        "    self.first_visits = np.full((self.env.shape[0], self.env.shape[1], self.env.nA), -1, dtype=np.int)\n",
        "  \n",
        "  def explore(self, state): \n",
        "    #here choose action according to the epsilon-soft policy\n",
        "    position = np.unravel_index(state, self.env.shape)\n",
        "    sample = np.random.rand()\n",
        "    sum = 0.\n",
        "    for action,p in enumerate(self.policy[position[0], position[1],:].flatten()):\n",
        "      sum += p\n",
        "      if sample <= sum:\n",
        "        break\n",
        "    return action\n",
        "\n",
        "  def act(self, state): \n",
        "    #here choose action according to the epsilon-soft policy\n",
        "    position = np.unravel_index(state, self.env.shape)\n",
        "    action = np.argmax(self.policy[position[0], position[1],:].flatten())\n",
        "    return action\n",
        "\n",
        "  def run_one_episode(self, exploring_starts):\n",
        "    icliff = 0 \n",
        "    episode = []\n",
        "    done = False\n",
        "    reward = 0\n",
        "    self.env.reset()\n",
        "    if exploring_starts:\n",
        "      state = np.random.randint(0,37)\n",
        "      action = np.random.randint(0,self.env.nA)\n",
        "      self.env.s = state\n",
        "    else:\n",
        "      state = self.env.s\n",
        "      action = self.explore(state)\n",
        "      \n",
        "    istep = 0\n",
        "    while (not done):\n",
        "      new_state, reward, done, _ = self.env.step(action)      \n",
        "      episode.append((state, action, reward))\n",
        "      position = np.unravel_index(state, self.env.shape)\n",
        "      if self.first_visits[position[0], position[1],action] == -1: \n",
        "        self.first_visits[position[0], position[1],action] = istep\n",
        "      new_position = np.unravel_index(new_state, self.env.shape)\n",
        "      if exploring_starts:\n",
        "        done = done or reward == -100 #self.env._cliff[tuple(new_position)]\n",
        "      istep += 1\n",
        "      if reward == -100: #(self.env._cliff[tuple(new_position)]):\n",
        "        icliff +=1 #print('Fell off the cliff')\n",
        "      state = self.env.s\n",
        "      action = self.explore(state)\n",
        "    print('Fell off the cliff ' + str(icliff) + ' times')\n",
        "    return episode\n",
        "\n",
        "  def iterate_policy(self, n, exploring_starts = False, epsilon=None):\n",
        "    if epsilon is not None:\n",
        "      self.epsilon = epsilon\n",
        "\n",
        "    for i in range(n):\n",
        "      self.first_visits = np.full((self.env.shape[0], self.env.shape[1], self.env.nA), -1, dtype=np.int)\n",
        "      episode = self.run_one_episode(exploring_starts)\n",
        "      G = 0\n",
        "      T = len(episode)\n",
        "      print('Episode length: ' + str(T))\n",
        "      for t in reversed(range(T)):\n",
        "        (state, action,  reward) = episode[t]\n",
        "        G = self.gamma * G + reward\n",
        "        position = np.unravel_index(state, self.env.shape)\n",
        "        if (self.first_visits[position[0], position[1], action] == t):\n",
        "          self.returns[position[0], position[1], action].append(G)\n",
        "          self.Q[position[0], position[1], action] = np.mean(self.returns[position[0], position[1], action])\n",
        "          A_star = np.argmax(self.Q[position[0], position[1], :])\n",
        "          for ip in range(self.env.nA): \n",
        "            self.policy[position[0], position[1], ip] = 1 - self.epsilon + self.epsilon / self.env.nA if ip == A_star else self.epsilon / self.env.nA\n",
        "      print('Iteration ' + str(i) + ' done.')"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmkpudqRbIsZ"
      },
      "source": [
        "agent = MonteCarloAgent(env,0.99,0.2)\n",
        "#perform value iteration\n",
        "agent.iterate_policy(100, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUVC2Ogrkg_K"
      },
      "source": [
        "agent.iterate_policy(1000,False, 0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LJKPwrbbV8m",
        "outputId": "74da8752-ad47-4905-ce06-1fa1233b835e"
      },
      "source": [
        "#evaluate agent and plot relevant qualities\n",
        "episode_reward=agent.evaluate()\n",
        "print(\"Episode return {}\".format(episode_reward))\n",
        "\n",
        "print(agent.Q[3,0])\n",
        "print(np.argmax(agent.policy, axis=2))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode return -17\n",
            "[ -17.194 -103.901  -20.081  -22.923]\n",
            "[[1 1 1 1 1 1 1 1 1 1 2 2]\n",
            " [1 0 3 0 0 3 0 1 1 1 1 2]\n",
            " [0 3 0 0 0 1 0 0 0 0 1 2]\n",
            " [0 0 1 3 3 3 1 3 1 0 2 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSNbjW2XcvO4"
      },
      "source": [
        "\n",
        "pagent = MonteCarloAgent(penv,0.99,0.2)\n",
        "#perform value iteration\n",
        "pagent.iterate_policy(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEgitf20qLLc"
      },
      "source": [
        "pagent.iterate_policy(1000, 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qjr4ALAKwFWm",
        "outputId": "d8c2d962-abdf-4e1d-9fb7-dc3f1b8fa037"
      },
      "source": [
        "#evaluate agent and plot relevant qualities\n",
        "episode_reward=pagent.evaluate()\n",
        "print(\"Episode return {}\".format(episode_reward))\n",
        "\n",
        "print(pagent.Q[1,0])\n",
        "print(np.argmax(pagent.policy, axis=2))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode return -100\n",
            "[-126.333 -126.803 -153.514 -110.973]\n",
            "[[1 0 1 2 2 3 1 0 2 1 2 3]\n",
            " [3 0 0 3 1 3 1 1 1 0 2 0]\n",
            " [0 3 1 1 0 1 0 0 3 0 1 2]\n",
            " [0 2 2 1 2 3 3 3 3 1 2 2]]\n"
          ]
        }
      ]
    }
  ]
}