{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Session 2 SMARTER_RL_1-Value_Policy_Iteration.ipynb","provenance":[{"file_id":"https://github.com/tstoyanov/smarter-rl/blob/main/SMARTER_RL_1_Value_Policy_Iteration.ipynb","timestamp":1635838236019}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"d4fPp_O1t95q"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"Aw7a3Ph5uAOK"},"source":["# Preliminaries\n","\n","This notebook lets you import a gym environment and set up an agent that acts within the environment. Your tasks is to then implement some of the classical RL algorithms: Value iteration and Policy iteration. Play attention to how you are going to evaluate your agents.\n","\n","First, we make sure that all dependencies are met"]},{"cell_type":"code","metadata":{"id":"qktWZzEdtxNN","executionInfo":{"status":"ok","timestamp":1635939943651,"user_tz":-60,"elapsed":3459,"user":{"displayName":"Peter Kimstrand","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08323163514993465067"}}},"source":["!pip install gym > /dev/null 2>&1"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_KLURrYHycqD"},"source":["# Testing the Gym environments\n","\n","Our next step is to import the gym package, create an environment, and make sure that we can use it."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZlAGn42vyoyN","executionInfo":{"status":"ok","timestamp":1635943329601,"user_tz":-60,"elapsed":359,"user":{"displayName":"Peter Kimstrand","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08323163514993465067"}},"outputId":"23e04945-266c-47b0-99e3-3d5c54f313c1"},"source":["import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","#create a cliff-walker\n","env = gym.make('CliffWalking-v0')\n","\n","#set the start state\n","state = env.reset()\n","#and take some random actions\n","for i in range(4):\n","  #render the environment\n","  env.render()\n","  \n","  #select a random action\n","  env.action_space.sample()\n","  #take a step and record next state, reward and termination\n","  state, reward, done, _ = env.step(action)\n","  print(\"Acted: {}\".format(action))\n","  print(\"State: {}\".format(state))\n","  print(\"Reward: {}\".format(reward))\n","  if done:\n","    #this environment only terminates once the goal is reached\n","    print(\"Done.\")\n","    break"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["o  o  o  o  o  o  o  o  o  o  o  o\n","o  o  o  o  o  o  o  o  o  o  o  o\n","o  o  o  o  o  o  o  o  o  o  o  o\n","x  C  C  C  C  C  C  C  C  C  C  T\n","\n","Acted: 3\n","State: 36\n","Reward: -1\n","o  o  o  o  o  o  o  o  o  o  o  o\n","o  o  o  o  o  o  o  o  o  o  o  o\n","o  o  o  o  o  o  o  o  o  o  o  o\n","x  C  C  C  C  C  C  C  C  C  C  T\n","\n","Acted: 3\n","State: 36\n","Reward: -1\n","o  o  o  o  o  o  o  o  o  o  o  o\n","o  o  o  o  o  o  o  o  o  o  o  o\n","o  o  o  o  o  o  o  o  o  o  o  o\n","x  C  C  C  C  C  C  C  C  C  C  T\n","\n","Acted: 3\n","State: 36\n","Reward: -1\n","o  o  o  o  o  o  o  o  o  o  o  o\n","o  o  o  o  o  o  o  o  o  o  o  o\n","o  o  o  o  o  o  o  o  o  o  o  o\n","x  C  C  C  C  C  C  C  C  C  C  T\n","\n","Acted: 3\n","State: 36\n","Reward: -1\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H3w4IfOE2Nck","executionInfo":{"status":"ok","timestamp":1635950921649,"user_tz":-60,"elapsed":330,"user":{"displayName":"Peter Kimstrand","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08323163514993465067"}},"outputId":"808e0c31-f7ef-4560-a0eb-ad857d1b0a06"},"source":["\n","env.P[47]"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: [(1.0, 35, -1, False)],\n"," 1: [(1.0, 47, -1, True)],\n"," 2: [(1.0, 47, -1, True)],\n"," 3: [(1.0, 36, -100, False)]}"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"UsWsM--l5D25"},"source":["# Defining an agent\n","\n","The next step is to define a class for our agents. We will derive from this class to later implement a Value Iteration, Policy Iteration and Monte Carlo control agent. The base class will only provide simple functionality."]},{"cell_type":"code","metadata":{"id":"fzIFdhOk5VoR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635942625164,"user_tz":-60,"elapsed":330,"user":{"displayName":"Peter Kimstrand","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08323163514993465067"}},"outputId":"ed379d60-4575-478d-bcd1-38b307137bb2"},"source":["class Agent :\n","  def __init__(self,env,discount_factor):\n","    self.env = env\n","    self.gamma = discount_factor\n","  \n","  def act(self, state):\n","    return self.env.action_space.sample() #returns a random action\n","\n","  def evaluate(self):\n","    # now let's test our random action agent\n","    n_steps = 100 #number of steps per episode\n","\n","    s = env.reset()\n","    episode_reward = 0\n","    \n","    for i in range(n_steps):\n","      s, r, d, _ = env.step(self.act(s))\n","      episode_reward += r\n","      if done:\n","        break\n","    return episode_reward\n","\n","#test simple evaluation function\n","random_agent = Agent(env,0.99)\n","episode_reward=random_agent.evaluate()\n","print(\"Episode return {}\".format(episode_reward))"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode return -1387\n"]}]},{"cell_type":"markdown","metadata":{"id":"hY-nBquS4S4j"},"source":["# Value Iteration Agent\n","\n","In this section you are to implement an agent that solves the environment, using Value Iteration"]},{"cell_type":"code","metadata":{"id":"NYLjRZPg4cQj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635950967730,"user_tz":-60,"elapsed":312,"user":{"displayName":"Peter Kimstrand","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08323163514993465067"}},"outputId":"65fbc474-3715-483c-a8c8-c9e287fb94ad"},"source":["class ValueAgent(Agent):\n","  def __init__(self,env,discount_factor,theta):\n","    super().__init__(env,discount_factor)\n","    #theta is an approximation error threshold\n","    self.theta = theta\n","    self.V = np.random.rand(self.env.observation_space.n)\n","    #set terminal state to 0\n","    self.V[-1] = 0 \n","  \n","  def act(self, state): \n","    #here choose action that would bring us to state with highest value\n","    # Select the action that has highest expected value\n"," \n","    values=[]\n","    for i in range(self.env.nA):\n","      _,next_state,_,_ = env.P[state][i][0]\n","      values.append(self.V[next_state])\n","    \n","    action = np.argmax(values)\n","    #print(action)\n","    return action\n","\n","   \n","\n","  def iterate(self):\n","    while(True):\n","    #for i in range(5):\n","      #print(self.V) \n","      delta = 0.0\n","      for state in range(self.env.nS-1):\n","        v = self.V[state]\n","        action = self.act(state)\n","        prob, next_state, reward, done = self.env.P[state][action][0]\n","\n","        #if not done:\n","        self.V[state] = prob * (reward + self.gamma*self.V[next_state])\n","        delta = max([delta, np.abs(v-self.V[state])])\n","      print(delta)\n","      if (delta < self.theta):\n","        print(delta)\n","        break\n","\n","\n","agent = ValueAgent(env,0.99,0.001)\n","#perform value iteration\n","agent.iterate()\n","#evaluate agent and plot relevant qualities\n","episode_reward=agent.evaluate()\n","print(\"Episode return {}\".format(episode_reward))\n","print(agent.V)"],"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["100.79795732681274\n","98.0562179234899\n","100.68085875665723\n","100.68085875665723\n","100.64052158641118\n","100.56518805330413\n","100.54953617277108\n","100.52488860386308\n","100.50963971782446\n","100.49454332064622\n","98.2567250796206\n","0.9041130297750186\n","0.8950718994772675\n","0.8861211804824958\n","0.8514155908691734\n","0.0\n","0.0\n","Episode return -100\n","[-13.12541872 -12.2478977  -11.36151283 -10.46617457  -9.5617925\n","  -8.64827525  -7.72553056  -6.79346521  -5.85198506  -4.90099501\n","  -3.940399    -2.9701     -12.2478977  -11.36151283 -10.46617457\n","  -9.5617925   -8.64827525  -7.72553056  -6.79346521  -5.85198506\n","  -4.90099501  -3.940399    -2.9701      -1.99       -11.36151283\n"," -10.46617457  -9.5617925   -8.64827525  -7.72553056  -6.79346521\n","  -5.85198506  -4.90099501  -3.940399    -2.9701      -1.99\n","  -1.         -12.2478977  -11.36151283 -10.46617457  -9.5617925\n","  -8.64827525  -7.72553056  -6.79346521  -5.85198506  -4.90099501\n","  -3.940399    -1.           0.        ]\n"]}]},{"cell_type":"markdown","metadata":{"id":"zDDxB_GN__Tw"},"source":["# Policy Iteration Agent\n","Follow the same procedure for implementing a policy iteration agent"]},{"cell_type":"code","metadata":{"id":"8gqQ38UqARau"},"source":["#code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u69f8gyXAGN_"},"source":["#Monte Carlo control agent\n","Follow the same procedure for implementing a Monte Carlo control agent"]},{"cell_type":"code","metadata":{"id":"2KwYvXP4ASSJ"},"source":["#code here"],"execution_count":null,"outputs":[]}]}