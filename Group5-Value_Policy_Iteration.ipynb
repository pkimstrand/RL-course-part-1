{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Session 2 SMARTER_RL_1-Value_Policy_Iteration.ipynb","provenance":[{"file_id":"https://github.com/tstoyanov/smarter-rl/blob/main/SMARTER_RL_1_Value_Policy_Iteration.ipynb","timestamp":1635838236019}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"d4fPp_O1t95q"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"Aw7a3Ph5uAOK"},"source":["# Preliminaries\n","\n","This notebook lets you import a gym environment and set up an agent that acts within the environment. Your tasks is to then implement some of the classical RL algorithms: Value iteration and Policy iteration. Play attention to how you are going to evaluate your agents.\n","\n","First, we make sure that all dependencies are met"]},{"cell_type":"code","metadata":{"id":"qktWZzEdtxNN","executionInfo":{"status":"ok","timestamp":1635939943651,"user_tz":-60,"elapsed":3459,"user":{"displayName":"Peter Kimstrand","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08323163514993465067"}}},"source":["!pip install gym > /dev/null 2>&1"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_KLURrYHycqD"},"source":["# Testing the Gym environments\n","\n","Our next step is to import the gym package, create an environment, and make sure that we can use it."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZlAGn42vyoyN","executionInfo":{"status":"ok","timestamp":1635943329601,"user_tz":-60,"elapsed":359,"user":{"displayName":"Peter Kimstrand","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08323163514993465067"}},"outputId":"23e04945-266c-47b0-99e3-3d5c54f313c1"},"source":["import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","#create a cliff-walker\n","env = gym.make('CliffWalking-v0')\n","\n","#set the start state\n","state = env.reset()\n","#and take some random actions\n","for i in range(4):\n","  #render the environment\n","  env.render()\n","  \n","  #select a random action\n","  env.action_space.sample()\n","  #take a step and record next state, reward and termination\n","  state, reward, done, _ = env.step(action)\n","  print(\"Acted: {}\".format(action))\n","  print(\"State: {}\".format(state))\n","  print(\"Reward: {}\".format(reward))\n","  if done:\n","    #this environment only terminates once the goal is reached\n","    print(\"Done.\")\n","    break"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["o  o  o  o  o  o  o  o  o  o  o  o\n","o  o  o  o  o  o  o  o  o  o  o  o\n","o  o  o  o  o  o  o  o  o  o  o  o\n","x  C  C  C  C  C  C  C  C  C  C  T\n","\n","Acted: 3\n","State: 36\n","Reward: -1\n","o  o  o  o  o  o  o  o  o  o  o  o\n","o  o  o  o  o  o  o  o  o  o  o  o\n","o  o  o  o  o  o  o  o  o  o  o  o\n","x  C  C  C  C  C  C  C  C  C  C  T\n","\n","Acted: 3\n","State: 36\n","Reward: -1\n","o  o  o  o  o  o  o  o  o  o  o  o\n","o  o  o  o  o  o  o  o  o  o  o  o\n","o  o  o  o  o  o  o  o  o  o  o  o\n","x  C  C  C  C  C  C  C  C  C  C  T\n","\n","Acted: 3\n","State: 36\n","Reward: -1\n","o  o  o  o  o  o  o  o  o  o  o  o\n","o  o  o  o  o  o  o  o  o  o  o  o\n","o  o  o  o  o  o  o  o  o  o  o  o\n","x  C  C  C  C  C  C  C  C  C  C  T\n","\n","Acted: 3\n","State: 36\n","Reward: -1\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H3w4IfOE2Nck","executionInfo":{"status":"ok","timestamp":1635950921649,"user_tz":-60,"elapsed":330,"user":{"displayName":"Peter Kimstrand","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08323163514993465067"}},"outputId":"808e0c31-f7ef-4560-a0eb-ad857d1b0a06"},"source":["\n","env.P[47]"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: [(1.0, 35, -1, False)],\n"," 1: [(1.0, 47, -1, True)],\n"," 2: [(1.0, 47, -1, True)],\n"," 3: [(1.0, 36, -100, False)]}"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"UsWsM--l5D25"},"source":["# Defining an agent\n","\n","The next step is to define a class for our agents. We will derive from this class to later implement a Value Iteration, Policy Iteration and Monte Carlo control agent. The base class will only provide simple functionality."]},{"cell_type":"code","metadata":{"id":"fzIFdhOk5VoR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635955926404,"user_tz":-60,"elapsed":7,"user":{"displayName":"Peter Kimstrand","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08323163514993465067"}},"outputId":"abb66b7a-ecdc-4ee3-e293-36332ba055a2"},"source":["class Agent :\n","  def __init__(self,env,discount_factor):\n","    self.env = env\n","    self.gamma = discount_factor\n","  \n","  def act(self, state):\n","    return self.env.action_space.sample() #returns a random action\n","\n","  def evaluate(self):\n","    # now let's test our random action agent\n","    n_steps = 100 #number of steps per episode\n","\n","    s = env.reset()\n","    episode_reward = 0\n","    \n","    for i in range(n_steps):\n","      s, r, d, _ = env.step(self.act(s))\n","      episode_reward += r\n","      if d:\n","        break\n","    return episode_reward\n","\n","#test simple evaluation function\n","random_agent = Agent(env,0.99)\n","episode_reward=random_agent.evaluate()\n","print(\"Episode return {}\".format(episode_reward))"],"execution_count":90,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode return -991\n"]}]},{"cell_type":"markdown","metadata":{"id":"hY-nBquS4S4j"},"source":["# Value Iteration Agent\n","\n","In this section you are to implement an agent that solves the environment, using Value Iteration"]},{"cell_type":"code","metadata":{"id":"NYLjRZPg4cQj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635955972467,"user_tz":-60,"elapsed":329,"user":{"displayName":"Peter Kimstrand","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08323163514993465067"}},"outputId":"247a0efd-a94f-49ec-dbde-dd8e69aafdb0"},"source":["class ValueAgent(Agent):\n","  def __init__(self,env,discount_factor,theta):\n","    super().__init__(env,discount_factor)\n","    #theta is an approximation error threshold\n","    self.theta = theta\n","    self.V = np.random.rand(self.env.observation_space.n)\n","    #set terminal state to 0\n","    #self.V[-11:-1] = -1000 \n","    self.V[-1] = 0\n","  \n","  def act(self, state): \n","    #here choose action that would bring us to state with highest value\n","    values=[]\n","    for i in range(self.env.nA):\n","      prob, next_state, reward, done = env.P[state][i][0]\n","      values.append(reward + self.gamma*self.V[next_state])\n","    \n","    action = np.argmax(values)\n","    if (type(action)==np.array): print (action)\n","    return action\n","\n","   \n","\n","  def iterate(self):\n","    while(True):\n","      delta = 0.0\n","      for state in range(self.env.nS-1):\n","        v = self.V[state]\n","        action = self.act(state)\n","        prob, next_state, reward, done = self.env.P[state][action][0]\n","\n","        self.V[state] = prob * (reward + self.gamma*self.V[next_state])\n","        delta = max([delta, np.abs(v-self.V[state])])\n","      print(delta)\n","      if (delta < self.theta):\n","        print(delta)\n","        break\n","\n","\n","agent = ValueAgent(env,0.99,0.001)\n","print(agent.V[:12])\n","print(agent.V[12:24])\n","print(agent.V[24:36])\n","print(agent.V[36:])\n","#perform value iteration\n","agent.iterate()\n","#evaluate agent and plot relevant qualities\n","episode_reward=agent.evaluate()\n","print(\"Episode return {}\".format(episode_reward))\n","np.set_printoptions(precision=3, linewidth=200)\n","print(agent.V[:12])\n","print(agent.V[12:24])\n","print(agent.V[24:36])\n","print(agent.V[36:])"],"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.973 0.137 0.582 0.797 0.941 0.984 0.975 0.036 0.883 0.534 0.198 0.094]\n","[0.444 0.485 0.765 0.721 0.705 0.466 0.246 0.906 0.864 0.603 0.95  0.425]\n","[0.927 0.895 0.412 0.393 0.335 0.179 0.617 0.869 0.147 0.095 0.142 0.139]\n","[0.077 0.205 0.133 0.208 0.267 0.584 0.98  0.245 0.978 0.796 0.847 0.   ]\n","2.874456488614557\n","1.7060187236901483\n","1.6889585364532465\n","1.672068951088714\n","1.6553482615778266\n","1.6387947789620485\n","1.6224068311724285\n","1.4254172414026236\n","1.4111630689885972\n","1.3970514382987114\n","0.91328134112843\n","0.9041485277171457\n","0.8951070424399745\n","0.886155972015576\n","0.8548599526441283\n","0.0\n","0.0\n","Episode return -13\n","[-13.125 -12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97 ]\n","[-12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97   -1.99 ]\n","[-11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97   -1.99   -1.   ]\n","[-12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -1.      0.   ]\n"]}]},{"cell_type":"markdown","metadata":{"id":"zDDxB_GN__Tw"},"source":["# Policy Iteration Agent\n","Follow the same procedure for implementing a policy iteration agent"]},{"cell_type":"code","metadata":{"id":"8gqQ38UqARau"},"source":["#code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u69f8gyXAGN_"},"source":["#Monte Carlo control agent\n","Follow the same procedure for implementing a Monte Carlo control agent"]},{"cell_type":"code","metadata":{"id":"2KwYvXP4ASSJ"},"source":["#code here"],"execution_count":null,"outputs":[]}]}