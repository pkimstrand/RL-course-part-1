{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 2 SMARTER_RL_1-Value_Policy_Iteration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4fPp_O1t95q"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw7a3Ph5uAOK"
      },
      "source": [
        "# Preliminaries\n",
        "\n",
        "This notebook lets you import a gym environment and set up an agent that acts within the environment. Your tasks is to then implement some of the classical RL algorithms: Value iteration and Policy iteration. Play attention to how you are going to evaluate your agents.\n",
        "\n",
        "First, we make sure that all dependencies are met"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qktWZzEdtxNN"
      },
      "source": [
        "!pip install gym > /dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KLURrYHycqD"
      },
      "source": [
        "# Testing the Gym environments\n",
        "\n",
        "Our next step is to import the gym package, create an environment, and make sure that we can use it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlAGn42vyoyN",
        "outputId": "9871bdcf-1ec0-4b28-c680-f0652108ef8a"
      },
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#create a cliff-walker\n",
        "env = gym.make('CliffWalking-v0')\n",
        "\n",
        "#set the start state\n",
        "state = env.reset()\n",
        "#and take some random actions\n",
        "for i in range(4):\n",
        "  #render the environment\n",
        "  env.render()\n",
        "  \n",
        "  #select a random action\n",
        "  action = env.action_space.sample()\n",
        "  #take a step and record next state, reward and termination\n",
        "  state, reward, done, _ = env.step(action)\n",
        "  print(\"Acted: {}\".format(action))\n",
        "  print(\"State: {}\".format(state))\n",
        "  print(\"Reward: {}\".format(reward))\n",
        "  if done:\n",
        "    #this environment only terminates once the goal is reached\n",
        "    print(\"Done.\")\n",
        "    break"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 1\n",
            "State: 36\n",
            "Reward: -100\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 2\n",
            "State: 36\n",
            "Reward: -1\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 0\n",
            "State: 24\n",
            "Reward: -1\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 0\n",
            "State: 12\n",
            "Reward: -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsWsM--l5D25"
      },
      "source": [
        "# Defining an agent\n",
        "\n",
        "The next step is to define a class for our agents. We will derive from this class to later implement a Value Iteration, Policy Iteration and Monte Carlo control agent. The base class will only provide simple functionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzIFdhOk5VoR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "537f916b-63b0-49f4-9be7-fe431a05c62d"
      },
      "source": [
        "class Agent :\n",
        "  def __init__(self,env,discount_factor):\n",
        "    self.env = env\n",
        "    self.gamma = discount_factor\n",
        "  \n",
        "  def act(self, state):\n",
        "    return self.env.action_space.sample() #returns a random action\n",
        "\n",
        "  def evaluate(self):\n",
        "    # now let's test our random action agent\n",
        "    n_steps = 100 #number of steps per episode\n",
        "\n",
        "    s = env.reset()\n",
        "    episode_reward = 0\n",
        "    \n",
        "    for i in range(n_steps):\n",
        "      s, r, d, _ = env.step(self.act(s))\n",
        "      episode_reward += r\n",
        "      if d:\n",
        "        break\n",
        "    return episode_reward\n",
        "\n",
        "#test simple evaluation function\n",
        "random_agent = Agent(env,0.99)\n",
        "episode_reward=random_agent.evaluate()\n",
        "print(\"Episode return {}\".format(episode_reward))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode return -1387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY-nBquS4S4j"
      },
      "source": [
        "# Value Iteration Agent\n",
        "\n",
        "In this section you are to implement an agent that solves the environment, using Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYLjRZPg4cQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf49b0eb-46c5-4c86-acdb-2e4d576f84c9"
      },
      "source": [
        "class ValueAgent(Agent):\n",
        "  def __init__(self,env,discount_factor,theta):\n",
        "    super().__init__(env,discount_factor)\n",
        "    #theta is an approximation error threshold\n",
        "    self.theta = theta\n",
        "    self.V = np.random.rand(self.env.shape[0], self.env.shape[1])\n",
        "    #set terminal state to 0\n",
        "    self.V[-1,-1] = 0\n",
        "  \n",
        "  def act(self, state): \n",
        "    #here choose action that would bring us to state with highest value\n",
        "    values=[]\n",
        "    for i in range(self.env.nA):\n",
        "      v=0\n",
        "      for j in range(len(self.env.P[state][i])):\n",
        "        prob, next_state, reward, done = self.env.P[state][i][j]\n",
        "        next_position = np.unravel_index(next_state, self.env.shape)\n",
        "        v += prob*(reward + self.gamma*self.V[next_position])\n",
        "      values.append(v)\n",
        "    \n",
        "    action = np.argmax(values)\n",
        "    if (type(action)==np.array): print (action)\n",
        "    return action\n",
        "\n",
        "   \n",
        "\n",
        "  def iterate(self):\n",
        "    while(True):\n",
        "      delta = 0.0\n",
        "      for state in range(self.env.nS-1):\n",
        "        position = np.unravel_index(state, self.env.shape)\n",
        "        v = self.V[position]\n",
        "        action = self.act(state)\n",
        "        value = 0\n",
        "        for j in range(len(self.env.P[state][action])):\n",
        "          prob, next_state, reward, done = self.env.P[state][action][j]\n",
        "          next_position = np.unravel_index(next_state, self.env.shape)\n",
        "          value += prob * (reward + self.gamma*self.V[next_position])\n",
        "        self.V[position] = value\n",
        "        delta = max([delta, np.abs(v-self.V[position])])\n",
        "      print(delta)\n",
        "      if (delta < self.theta):\n",
        "        print(delta)\n",
        "        break\n",
        "\n",
        "np.set_printoptions(precision=3, linewidth=200)\n",
        "\n",
        "agent = ValueAgent(env,0.99,0.001)\n",
        "print(agent.V)\n",
        "#perform value iteration\n",
        "agent.iterate()\n",
        "#evaluate agent and plot relevant qualities\n",
        "episode_reward=agent.evaluate()\n",
        "print(\"Episode return {}\".format(episode_reward))\n",
        "\n",
        "print(agent.V)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.476 0.571 0.513 0.335 0.743 0.247 0.739 0.995 0.87  0.832 0.03  0.899]\n",
            " [0.29  0.227 0.829 0.887 0.814 0.712 0.498 0.345 0.842 0.165 0.627 0.349]\n",
            " [0.094 0.943 0.962 0.671 0.552 0.714 0.202 0.049 0.846 0.651 0.645 0.225]\n",
            " [0.23  0.206 0.154 0.136 0.153 0.876 0.216 0.877 0.379 0.233 0.405 0.   ]]\n",
            "2.667789823092024\n",
            "1.499011267147722\n",
            "1.484021154476245\n",
            "1.4691809429314824\n",
            "1.4544891335021677\n",
            "1.352347158321468\n",
            "1.3388236867382535\n",
            "1.325435449870871\n",
            "1.3121810953721615\n",
            "1.2990592844184388\n",
            "1.1747365045306069\n",
            "0.904243227670408\n",
            "0.8952007953937038\n",
            "0.8862487874397669\n",
            "0.8640486796390174\n",
            "0.0\n",
            "0.0\n",
            "Episode return -13\n",
            "[[-13.125 -12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97 ]\n",
            " [-12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97   -1.99 ]\n",
            " [-11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97   -1.99   -1.   ]\n",
            " [-12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -1.      0.   ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDDxB_GN__Tw"
      },
      "source": [
        "# Policy Iteration Agent\n",
        "Follow the same procedure for implementing a policy iteration agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gqQ38UqARau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92fe3313-79fe-411b-f734-44d8ed041ca4"
      },
      "source": [
        "class PolicyAgent(Agent):\n",
        "  def __init__(self,env,discount_factor,theta):\n",
        "    super().__init__(env,discount_factor)\n",
        "    #theta is an approximation error threshold\n",
        "    self.theta = theta\n",
        "    self.V = np.random.rand(self.env.shape[0], self.env.shape[1])\n",
        "    #set terminal state to 0\n",
        "    self.V[-1,-1] = 0\n",
        "    self.policy = np.random.randint(4, size=self.env.shape)\n",
        "  \n",
        "  def act(self, state): \n",
        "    #here choose action that would bring us to state with highest value\n",
        "    values=[]\n",
        "    for i in range(self.env.nA):\n",
        "      v=0\n",
        "      for j in range(len(self.env.P[state][i])):\n",
        "        prob, next_state, reward, done = self.env.P[state][i][j]\n",
        "        next_position = np.unravel_index(next_state, self.env.shape)\n",
        "        v += prob * (reward + self.gamma*self.V[next_position])\n",
        "      values.append(v)\n",
        "    \n",
        "    action = np.argmax(values)\n",
        "    if (type(action)==np.array): print (action)\n",
        "    return action\n",
        "\n",
        "   \n",
        "\n",
        "  def evaluate_policy(self):\n",
        "    while(True):\n",
        "      delta = 0.0\n",
        "      for state in range(self.env.nS-1):\n",
        "        position = np.unravel_index(state, self.env.shape)\n",
        "        v = self.V[position]\n",
        "        action = self.policy[position]\n",
        "        value = 0\n",
        "        for j in range(len(self.env.P[state][action])):\n",
        "          prob, next_state, reward, done = self.env.P[state][action][j]\n",
        "          next_position = np.unravel_index(next_state, self.env.shape)\n",
        "          value += prob * (reward + self.gamma*self.V[next_position])\n",
        "        self.V[position] = value\n",
        "        delta = max([delta, np.abs(v-self.V[position])])\n",
        "      #print(delta)\n",
        "      if (delta < self.theta):\n",
        "        #print(delta)\n",
        "        break\n",
        "\n",
        "  def improve(self):\n",
        "    policy_stable = True\n",
        "    #for i in range(200):\n",
        "    while(True):\n",
        "      policy_stable = True\n",
        "      for state in range(self.env.nS-1):\n",
        "        position = np.unravel_index(state, self.env.shape)\n",
        "        old_action = self.policy[position]\n",
        "        self.policy[position] = self.act(state)\n",
        "        if not (self.policy[position] == old_action):\n",
        "          policy_stable = False\n",
        "      if policy_stable:\n",
        "        break\n",
        "      else:\n",
        "        self.evaluate_policy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "agent = PolicyAgent(env,0.99,0.001)\n",
        "print(agent.V)\n",
        "\n",
        "print(agent.policy)\n",
        "#perform value iteration\n",
        "agent.improve()\n",
        "#evaluate agent and plot relevant qualities\n",
        "episode_reward=agent.evaluate()\n",
        "print(\"Episode return {}\".format(episode_reward))\n",
        "np.set_printoptions(precision=3, linewidth=200)\n",
        "print(agent.V)\n",
        "\n",
        "print(agent.policy)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.689 0.788 0.312 0.997 0.065 0.295 0.505 0.212 0.324 0.971 0.08  0.23 ]\n",
            " [0.599 0.01  0.092 0.852 0.212 0.353 0.206 0.345 0.397 0.755 0.717 0.149]\n",
            " [0.186 0.202 0.738 0.981 0.657 0.297 0.499 0.812 0.032 0.438 0.139 0.73 ]\n",
            " [0.77  0.891 0.183 0.651 0.525 0.851 0.255 0.392 0.639 0.996 0.88  0.   ]]\n",
            "[[0 3 2 3 2 1 3 1 1 1 3 0]\n",
            " [3 0 3 1 2 3 1 3 3 2 1 0]\n",
            " [3 3 1 1 2 0 3 0 1 3 3 1]\n",
            " [0 2 2 1 1 3 2 0 2 1 0 0]]\n",
            "Episode return -13\n",
            "[[-13.125 -12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97 ]\n",
            " [-12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97   -1.99 ]\n",
            " [-11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -2.97   -1.99   -1.   ]\n",
            " [-12.248 -11.362 -10.466  -9.562  -8.648  -7.726  -6.793  -5.852  -4.901  -3.94   -1.      0.   ]]\n",
            "[[1 1 1 1 1 1 1 1 1 1 1 2]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 2]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 2]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6TkdlfzPpcI"
      },
      "source": [
        "from gym.envs.toy_text.cliffwalking import CliffWalkingEnv\n",
        "import copy\n",
        "\n",
        "class NonDCliffWalkingEnv(CliffWalkingEnv):\n",
        "  def __init__(self):\n",
        "    super(NonDCliffWalkingEnv, self).__init__()\n",
        "  \n",
        "  def _calculate_transition_prob(self, current, delta):\n",
        "    \"\"\"\n",
        "    Determine the outcome for an action. Transition Prob is always 1.0.\n",
        "    :param current: Current position on the grid as (row, col)\n",
        "    :param delta: Change in position for transition\n",
        "    :return: (1.0, new_state, reward, done)\n",
        "    \"\"\"\n",
        "    transitions = []\n",
        "    terminal_state = (self.shape[0] - 1, self.shape[1] - 1)\n",
        "    deltas = [delta, [2*delta[0], 2*delta[1]]]\n",
        "    zero_index = delta.index(0)\n",
        "    delta_off = copy.copy(delta)\n",
        "    delta_off[zero_index] = 1\n",
        "    deltas.append(copy.copy(delta_off))\n",
        "    delta_off[zero_index] = -1\n",
        "    deltas.append(delta_off)\n",
        "    probabilities = [0.85,0.05,0.05,0.05]\n",
        "\n",
        "    \n",
        "    for d,p in zip(deltas, probabilities):\n",
        "      new_position = np.array(current) + np.array(d)\n",
        "      new_position = self._limit_coordinates(new_position).astype(int)\n",
        "      new_state = self.start_state_index if self._cliff[tuple(new_position)] else np.ravel_multi_index(tuple(new_position), self.shape)\n",
        "      reward = -100 if self._cliff[tuple(new_position)] else -1\n",
        "      is_done = tuple(new_position) == terminal_state\n",
        "      #print(tuple(new_position), self._cliff[tuple(new_position)])\n",
        "      transitions.append((p, new_state, reward, is_done))\n",
        "    \n",
        "    return transitions\n",
        "  "
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7b4tF5uHpT6",
        "outputId": "7ddc130b-0338-4c9c-b482-6b4e6dfde8e9"
      },
      "source": [
        "penv = NonDCliffWalkingEnv()\n",
        "len(penv.P[33][1])"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKYehYTOIshW",
        "outputId": "75880f37-88d8-4efc-d517-d8807f4bd704"
      },
      "source": [
        "agent = ValueAgent(penv,0.99,0.001)\n",
        "print(agent.V)\n",
        "#perform value iteration\n",
        "agent.iterate()\n",
        "#evaluate agent and plot relevant qualities\n",
        "episode_reward=agent.evaluate()\n",
        "print(\"Episode return {}\".format(episode_reward))\n",
        "\n",
        "print(agent.V)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.73  0.713 0.755 0.991 0.766 0.739 0.006 0.152 0.395 0.403 0.809 0.872]\n",
            " [0.197 0.632 0.583 0.153 0.231 0.704 0.908 0.471 0.28  0.971 0.037 0.231]\n",
            " [0.052 0.859 0.236 0.555 0.752 0.273 0.107 0.957 0.596 0.406 0.178 0.548]\n",
            " [0.149 0.223 0.366 0.194 0.879 0.812 0.3   0.122 0.042 0.456 0.514 0.   ]]\n",
            "3.195485307266794\n",
            "1.8264885663777233\n",
            "1.7086905478232992\n",
            "1.6039457938409836\n",
            "1.5112050822321326\n",
            "1.4285616351882844\n",
            "1.3548277899922239\n",
            "1.289043803178373\n",
            "1.2308974382021711\n",
            "0.9531310043923362\n",
            "0.9427318156800428\n",
            "0.9324878752455739\n",
            "0.9223892417134429\n",
            "0.9124275115706908\n",
            "0.9025954662639961\n",
            "0.8926118351231072\n",
            "0.8823241819631686\n",
            "0.8681937178627948\n",
            "0.8400751102013579\n",
            "0.4156893480130037\n",
            "0.18851470705277507\n",
            "0.0525691208825414\n",
            "0.04000626839519583\n",
            "0.03921831509546081\n",
            "0.03878952333653274\n",
            "0.038369180270020564\n",
            "0.03795403485254667\n",
            "0.037540632941261975\n",
            "0.03711305977252621\n",
            "0.03656051919224623\n",
            "0.03513551090035705\n",
            "0.03017640164746993\n",
            "0.019116988632095655\n",
            "0.007472309832273538\n",
            "0.0023964026376859238\n",
            "0.0016643727207750203\n",
            "0.0015922999312820707\n",
            "0.0015722930001587088\n",
            "0.0015551231709558522\n",
            "0.0015378920240500804\n",
            "0.0015186638567143973\n",
            "0.0014886392038704344\n",
            "0.0014179393039306376\n",
            "0.0012449778004253176\n",
            "0.0009213144338389156\n",
            "0.0009213144338389156\n",
            "Episode return -15\n",
            "[[-17.381 -16.592 -15.798 -14.999 -14.196 -13.389 -12.578 -11.765 -10.949 -10.131  -9.31   -8.445]\n",
            " [-17.326 -16.486 -15.635 -14.772 -13.897 -13.009 -12.108 -11.192 -10.262  -9.31   -8.445  -7.504]\n",
            " [-18.114 -17.326 -16.486 -15.635 -14.773 -13.897 -13.009 -12.108 -11.192 -10.262  -7.504  -6.883]\n",
            " [-18.855 -18.109 -17.279 -16.436 -15.581 -14.714 -13.834 -12.941 -12.034 -11.022  -1.341   0.   ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea0XuckqJI65",
        "outputId": "b1b69a1b-2fb9-43eb-ed56-984fedb04a8d"
      },
      "source": [
        "pagent = PolicyAgent(penv,0.99,0.001)\n",
        "print(agent.V)\n",
        "\n",
        "print(pagent.policy)\n",
        "#perform value iteration\n",
        "pagent.improve()\n",
        "#evaluate agent and plot relevant qualities\n",
        "episode_reward=pagent.evaluate()\n",
        "print(\"Episode return {}\".format(episode_reward))\n",
        "\n",
        "print(pagent.V)\n",
        "\n",
        "print(pagent.policy)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-17.381 -16.592 -15.798 -14.999 -14.196 -13.389 -12.578 -11.765 -10.949 -10.131  -9.31   -8.445]\n",
            " [-17.326 -16.486 -15.635 -14.772 -13.897 -13.009 -12.108 -11.192 -10.262  -9.31   -8.445  -7.504]\n",
            " [-18.114 -17.326 -16.486 -15.635 -14.773 -13.897 -13.009 -12.108 -11.192 -10.262  -7.504  -6.883]\n",
            " [-18.855 -18.109 -17.279 -16.436 -15.581 -14.714 -13.834 -12.941 -12.034 -11.022  -1.341   0.   ]]\n",
            "[[1 2 0 1 2 2 1 3 0 0 3 2]\n",
            " [0 2 1 0 0 2 0 2 1 2 0 0]\n",
            " [1 2 1 3 2 3 0 3 0 2 1 0]\n",
            " [1 2 0 3 1 0 0 1 3 3 2 1]]\n",
            "Episode return -15\n",
            "[[-17.384 -16.594 -15.8   -15.001 -14.197 -13.39  -12.58  -11.766 -10.95  -10.132  -9.311  -8.446]\n",
            " [-17.328 -16.488 -15.637 -14.774 -13.899 -13.011 -12.109 -11.193 -10.263  -9.311  -8.446  -7.505]\n",
            " [-18.116 -17.328 -16.488 -15.637 -14.774 -13.899 -13.011 -12.109 -11.193 -10.263  -7.505  -6.883]\n",
            " [-18.856 -18.11  -17.28  -16.437 -15.582 -14.715 -13.835 -12.942 -12.035 -11.023  -1.341   0.   ]]\n",
            "[[1 1 1 1 1 1 1 1 1 1 2 2]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 2]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 2]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u69f8gyXAGN_"
      },
      "source": [
        "#Monte Carlo control agent\n",
        "Follow the same procedure for implementing a Monte Carlo control agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KwYvXP4ASSJ"
      },
      "source": [
        "#code here"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}